// RUN: triton-shared-opt --collapse-shape --split-input-file %s | FileCheck %s

// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py

// The script is designed to make adding checks to
// a test case fast, it is *not* designed to be authoritative
// about what constitutes a good test! The CHECK should be
// minimized and named to reflect the test intent.

module {
// CHECK-LABEL:   func.func @fill() -> tensor<1x1x1x1x1x2x1xi32> {
// CHECK:           %[[VAL_0:.*]] = arith.constant 1 : i32
// CHECK:           %[[VAL_1:.*]] = tensor.empty() : tensor<1x1x1x1x1x2x1xi32>
// CHECK:           %[[VAL_2:.*]] = tensor.collapse_shape %[[VAL_1]] {{\[\[}}0, 1, 2, 3, 4, 5, 6]] : tensor<1x1x1x1x1x2x1xi32> into tensor<2xi32>
// CHECK:           %[[VAL_3:.*]] = linalg.fill ins(%[[VAL_0]] : i32) outs(%[[VAL_2]] : tensor<2xi32>) -> tensor<2xi32>
// CHECK:           %[[VAL_4:.*]] = tensor.expand_shape %[[VAL_3]] {{\[\[}}0, 1, 2, 3, 4, 5, 6]] output_shape [1, 1, 1, 1, 1, 2, 1] : tensor<2xi32> into tensor<1x1x1x1x1x2x1xi32>
// CHECK:           return %[[VAL_4]] : tensor<1x1x1x1x1x2x1xi32>
// CHECK:         }

// CHECK-LABEL:   func.func @fill2() -> memref<32x32xf32> {
// CHECK:           %[[VAL_0:.*]] = arith.constant 0xFF800000 : f32
// CHECK:           %[[VAL_1:.*]] = memref.alloc() : memref<32x32xf32>
// CHECK:           %[[VAL_2:.*]] = memref.collapse_shape %[[VAL_1]] {{\[\[}}0, 1]] : memref<32x32xf32> into memref<1024xf32>
// CHECK:           linalg.fill ins(%[[VAL_0]] : f32) outs(%[[VAL_2]] : memref<1024xf32>)
// CHECK:           return %[[VAL_1]] : memref<32x32xf32>
// CHECK:         }
  func.func @fill() -> tensor<1x1x1x1x1x2x1xi32> {
    %c1_i32 = arith.constant 1 : i32
    %0 = tensor.empty() : tensor<1x1x1x1x1x2x1xi32>
    %1 = linalg.fill ins(%c1_i32 : i32) outs(%0 : tensor<1x1x1x1x1x2x1xi32>) -> tensor<1x1x1x1x1x2x1xi32>
    return %1 : tensor<1x1x1x1x1x2x1xi32>
  }
  func.func @fill2() -> memref<32x32xf32> {
    %cst = arith.constant 0xFF800000 : f32
    %alloc = memref.alloc() : memref<32x32xf32>
    linalg.fill ins(%cst : f32) outs(%alloc : memref<32x32xf32>)
    return %alloc : memref<32x32xf32>
  }
}

// -----
// CHECK: #[[$ATTR_0:.+]] = affine_map<(d0, d1) -> (0, d1)>
// CHECK: #[[$ATTR_1:.+]] = affine_map<(d0, d1) -> (d0, d1)>
// CHECK-LABEL:   func.func @broadcast() -> tensor<2x2x2x2x2x2x2x2x2x2xi32> {
// CHECK:           %[[VAL_0:.*]] = tensor.empty() : tensor<1x1x1x1x1x1x1x1x2x2xi32>
// CHECK:           %[[VAL_1:.*]] = tensor.empty() : tensor<2x2x2x2x2x2x2x2x2x2xi32>
// CHECK:           %[[VAL_2:.*]] = tensor.collapse_shape %[[VAL_0]] {{\[\[}}0, 1, 2, 3, 4, 5, 6, 7], [8, 9]] : tensor<1x1x1x1x1x1x1x1x2x2xi32> into tensor<1x4xi32>
// CHECK:           %[[VAL_3:.*]] = tensor.collapse_shape %[[VAL_1]] {{\[\[}}0, 1, 2, 3, 4, 5, 6, 7], [8, 9]] : tensor<2x2x2x2x2x2x2x2x2x2xi32> into tensor<256x4xi32>
// CHECK:           %[[VAL_4:.*]] = linalg.generic {indexing_maps = [#[[$ATTR_0]], #[[$ATTR_1]]], iterator_types = ["parallel", "parallel"]} ins(%[[VAL_2]] : tensor<1x4xi32>) outs(%[[VAL_3]] : tensor<256x4xi32>) attrs =  {broadcastDims = array<i64: 0>} {
// CHECK:           ^bb0(%[[VAL_5:.*]]: i32, %[[VAL_6:.*]]: i32):
// CHECK:             linalg.yield %[[VAL_5]] : i32
// CHECK:           } -> tensor<256x4xi32>
// CHECK:           %[[VAL_7:.*]] = tensor.expand_shape %[[VAL_4]] {{\[\[}}0, 1, 2, 3, 4, 5, 6, 7], [8, 9]] output_shape [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] : tensor<256x4xi32> into tensor<2x2x2x2x2x2x2x2x2x2xi32>
// CHECK:           return %[[VAL_7]] : tensor<2x2x2x2x2x2x2x2x2x2xi32>
// CHECK:         }
#map = affine_map<(d0, d1, d2, d3, d4, d5, d6, d7, d8, d9) -> (0, 0, 0, 0, 0, 0, 0, 0, d8, d9)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5, d6, d7, d8, d9) -> (d0, d1, d2, d3, d4, d5, d6, d7, d8, d9)>
module {
  func.func @broadcast() -> tensor<2x2x2x2x2x2x2x2x2x2xi32> {
    %0 = tensor.empty() : tensor<1x1x1x1x1x1x1x1x2x2xi32>
    %1 = tensor.empty() : tensor<2x2x2x2x2x2x2x2x2x2xi32>
    %2 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%0 : tensor<1x1x1x1x1x1x1x1x2x2xi32>) outs(%1 : tensor<2x2x2x2x2x2x2x2x2x2xi32>) attrs =  {broadcastDims = array<i64: 0, 1, 2, 3, 4, 5, 6, 7>} {
    ^bb0(%in: i32, %out: i32):
      linalg.yield %in : i32
    } -> tensor<2x2x2x2x2x2x2x2x2x2xi32>
    return %2 : tensor<2x2x2x2x2x2x2x2x2x2xi32>
  }
}

// -----
module {
// CHECK-LABEL:   func.func @transpose() -> tensor<2x2x2x2x2x2x2x2x2x2xi64> {
// CHECK:           %[[VAL_0:.*]] = tensor.empty() : tensor<2x2x2x2x2x2x2x2x2x2xi64>
// CHECK:           %[[VAL_1:.*]] = tensor.empty() : tensor<2x2x2x2x2x2x2x2x2x2xi64>
// CHECK:           %[[VAL_2:.*]] = tensor.collapse_shape %[[VAL_0]] {{\[\[}}0, 1, 2, 3, 4, 5, 6, 7], [8], [9]] : tensor<2x2x2x2x2x2x2x2x2x2xi64> into tensor<256x2x2xi64>
// CHECK:           %[[VAL_3:.*]] = tensor.collapse_shape %[[VAL_1]] {{\[\[}}0, 1, 2, 3, 4, 5, 6, 7], [8], [9]] : tensor<2x2x2x2x2x2x2x2x2x2xi64> into tensor<256x2x2xi64>
// CHECK:           %[[VAL_4:.*]] = linalg.transpose ins(%[[VAL_2]] : tensor<256x2x2xi64>) outs(%[[VAL_3]] : tensor<256x2x2xi64>) permutation = [0, 2, 1]
// CHECK:           %[[VAL_5:.*]] = tensor.expand_shape %[[VAL_4]] {{\[\[}}0, 1, 2, 3, 4, 5, 6, 7], [8], [9]] output_shape [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] : tensor<256x2x2xi64> into tensor<2x2x2x2x2x2x2x2x2x2xi64>
// CHECK:           return %[[VAL_5]] : tensor<2x2x2x2x2x2x2x2x2x2xi64>
// CHECK:         }
  func.func @transpose() -> tensor<2x2x2x2x2x2x2x2x2x2xi64> {
    %0 = tensor.empty() : tensor<2x2x2x2x2x2x2x2x2x2xi64>
    %1 = tensor.empty() : tensor<2x2x2x2x2x2x2x2x2x2xi64>
    %transposed = linalg.transpose ins(%0 : tensor<2x2x2x2x2x2x2x2x2x2xi64>) outs(%1 : tensor<2x2x2x2x2x2x2x2x2x2xi64>) permutation = [0, 1, 2, 3, 4, 5, 6, 7, 9, 8] 
    return %transposed : tensor<2x2x2x2x2x2x2x2x2x2xi64>
  }
}

// -----
module {
// CHECK-LABEL:   func.func @reduce() -> tensor<2x2x2x2x2x2x2x2x2xi64> {
// CHECK:           %[[VAL_0:.*]] = tensor.empty() : tensor<2x2x2x2x2x2x2x2x2x2xi64>
// CHECK:           %[[VAL_1:.*]] = tensor.empty() : tensor<2x2x2x2x2x2x2x2x2xi64>
// CHECK:           %[[VAL_2:.*]] = tensor.collapse_shape %[[VAL_0]] {{\[\[}}0, 1, 2, 3, 4, 5, 6, 7], [8], [9]] : tensor<2x2x2x2x2x2x2x2x2x2xi64> into tensor<256x2x2xi64>
// CHECK:           %[[VAL_3:.*]] = tensor.collapse_shape %[[VAL_1]] {{\[\[}}0, 1, 2, 3, 4, 5, 6, 7], [8]] : tensor<2x2x2x2x2x2x2x2x2xi64> into tensor<256x2xi64>
// CHECK:           %[[VAL_4:.*]] = linalg.reduce ins(%[[VAL_2]] : tensor<256x2x2xi64>) outs(%[[VAL_3]] : tensor<256x2xi64>) dimensions = [1]
// CHECK:             (%[[VAL_5:.*]]: i64, %[[VAL_6:.*]]: i64) {
// CHECK:               %[[VAL_7:.*]] = arith.xori %[[VAL_5]], %[[VAL_6]] : i64
// CHECK:               linalg.yield %[[VAL_7]] : i64
// CHECK:             }
// CHECK:           %[[VAL_8:.*]] = tensor.expand_shape %[[VAL_4]] {{\[\[}}0, 1, 2, 3, 4, 5, 6, 7], [8]] output_shape [2, 2, 2, 2, 2, 2, 2, 2, 2] : tensor<256x2xi64> into tensor<2x2x2x2x2x2x2x2x2xi64>
// CHECK:           return %[[VAL_8]] : tensor<2x2x2x2x2x2x2x2x2xi64>
// CHECK:         }
  func.func @reduce() -> tensor<2x2x2x2x2x2x2x2x2xi64> {
    %0 = tensor.empty() : tensor<2x2x2x2x2x2x2x2x2x2xi64>
    %1 = tensor.empty() : tensor<2x2x2x2x2x2x2x2x2xi64>
    %reduced = linalg.reduce ins(%0 : tensor<2x2x2x2x2x2x2x2x2x2xi64>) outs(%1 : tensor<2x2x2x2x2x2x2x2x2xi64>) dimensions = [8] 
      (%in: i64, %init: i64) {
        %2 = arith.xori %in, %init : i64
        linalg.yield %2 : i64
      }
    return %reduced : tensor<2x2x2x2x2x2x2x2x2xi64>
  }
}